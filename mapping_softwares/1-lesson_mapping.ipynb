{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;font-size: 40pt\">Mapping node</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "Objectives of this lesson:\n",
    "- explain how a 3D mapping node in ROS works\n",
    "- assist students in the construction of their first 3D map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this lesson, we are going to look at [ethzasl_icp_mapper](https://github.com/ethz-asl/ethzasl_icp_mapping/tree/reintegrate/master_into_indigo_devel/ethzasl_icp_mapper), which is a 3D mapping node in ROS. This node is written in C++ and uses the Iterative Closest Point (ICP) algorithm in the background to register point clouds. In the first part of the lesson, we will look at the way this node works. Then, we are going to build a 3D map using this mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pipeline\n",
    "Although it might look complicated, in reality, it is quite easy to understand how a 3D mapper works. Here is an illustration of what happens inside:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/simple_mapper.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "First, as input, the mapper takes a lidar point cloud and current [odometry](https://en.wikipedia.org/wiki/Odometry) information. The point cloud is used to build the map and to localize in it. The odometry information is used as a prior (i.e. an estimation) for localization. As output, the mapper produces a refined odometry as well as a map. The outputted map is used in the next iteration of mapping, where a new input point cloud and new odometry information are fed to the mapper.\n",
    "\n",
    "Let's look more carefully at the *Registration* box inside the mapper. As you've seen in the [previous module](../registration/0-overview.ipynb), a registration algorithm takes two point clouds and finds the transformation that minimizes the alignment error between them. Here, the registration algorithm takes two point clouds and a transformation. This transformation is only used to roughly pre-align the point clouds, to make the registration job easier. Then, the registration algorithm just has to fine-tune this alignment. The two input point clouds are the latest point cloud produced by the lidar and the current map. The output of this step is a transformation representing the precise localization of the lidar within the map.\n",
    "\n",
    "Now, let's have a look at the *Transformation* box. The only purpose of this step is to take the input point cloud and to align it to the map by applying the transformation computed previously. The output of this step is the transformed input point cloud.\n",
    "\n",
    "The last box remaining is *Merge and maintenance*. This step takes the current map and the transformed input point cloud and merges them. This step also includes maintenance jobs, such as identifying sections of the map which are not relevant (e.g. moving objects). The output of this step is the new current map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline\n",
    "Now that you understand the basics, let's add a two optional steps:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/complicated_mapper.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "The *Input filters* box is a pre-processing step. It applies a series of <tt>data filters</tt>, which were introduced in the [processing lesson](../registration/2-lesson_processing.ipynb) of the previous module, to the input point cloud. A common use case for this step is to remove portions of the vehicle seen by the lidar from input clouds. If those portions are not removed, there will be a trail of points on the path of the vehicle in the map. The output of this step is the filtered input point cloud.\n",
    "\n",
    "The *Map post filters* box is a post-processing step. It applies a series of <tt>data filters</tt> to the map produced at the end of an iteration. A common use case for this step is to remove the sections of the map that were marked as non-relevant in the *Merge and maintenance* step. The output of this step is the new current map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration files\n",
    "The ethzasl_icp_mapper node allows users to set many options to customize the mapping. With them, it is possible to set the [tf](../ros/4-lesson-ros-tf.ipynb) frame names, the sensor max range, the wanted resolution for the map, the probability of points being dynamic, etc. On the virtual machine provided for this lecture, all those options are set in the `~/percep3d_workspace/catkin_workspace/src/ethzasl_icp_mapping/ethzasl_icp_mapper/launch/husky/darpa_dynamic_mapper_ctu.launch` launch file. In those options, it is possible to provide three different configuration files. Those configuration files are used to control what is happening in the *Input filters*, *Registration* and *Map post filters* steps. On the virtual machine, those three configuration files are:\n",
    "- `~/percep3d_workspace/catkin_workspace/src/ethzasl_icp_mapping/ethzasl_icp_mapper/launch/husky/darpa_input_filters_ctu.yaml`\n",
    "- `~/percep3d_workspace/catkin_workspace/src/ethzasl_icp_mapping/ethzasl_icp_mapper/launch/husky/darpa_icp_param_ctu.yaml`\n",
    "- `~/percep3d_workspace/catkin_workspace/src/ethzasl_icp_mapping/ethzasl_icp_mapper/launch/husky/darpa_mapPost_filters_ctu.yaml`\n",
    "\n",
    "Here is an illustration of where those configuration files are used:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/mapper.png\" width=\"90%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input filters\n",
    "Here is the non-commented content of `darpa_input_filters_ctu.yaml`:\n",
    "```\n",
    "- BoundingBoxDataPointsFilter:\n",
    "    xMin: -0.3\n",
    "    xMax: 10\n",
    "    yMin: -0.1\n",
    "    yMax: 0.2\n",
    "    zMin: -0.2\n",
    "    zMax: 5\n",
    "    removeInside: 1\n",
    " \n",
    "- SurfaceNormalDataPointsFilter:\n",
    "    knn: 12\n",
    "    epsilon: 1.33\n",
    "    keepNormals: 1\n",
    "    keepDensities: 1\n",
    "    \n",
    "- MaxDensityDataPointsFilter:\n",
    "    maxDensity: 1000.0\n",
    "    \n",
    "- ShadowDataPointsFilter:\n",
    "    eps: 0.1\n",
    "    \n",
    "- SimpleSensorNoiseDataPointsFilter:\n",
    "    sensorType: 0\n",
    "\n",
    "- ObservationDirectionDataPointsFilter\n",
    "\n",
    "- OrientNormalsDataPointsFilter:\n",
    "    towardCenter: 1\n",
    "```\n",
    "The `BoundingBoxDataPointsFilter` is there to remove portions of the vehicle seen by the lidar. The `SurfaceNormalDataPointsFilter` is there to compute the density and normals associated to each point. The density information is needed because the point cloud is then filtered based on its density by the `MaxDensityDataPointsFilter`. As for the normals at each point, they must be computed since a Point-to-Plane error minimizer is used, as you will see in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICP parameters\n",
    "Here is the non-commented content of `darpa_icp_param_ctu.yaml`:\n",
    "```\n",
    "matcher:\n",
    "  KDTreeMatcher:\n",
    "    knn: 10\n",
    "    maxDist: 1.5\n",
    "    epsilon: 1\n",
    "\n",
    "outlierFilters:\n",
    "  - TrimmedDistOutlierFilter:\n",
    "    ratio: 0.80\n",
    "  - SurfaceNormalOutlierFilter:\n",
    "    maxAngle: 0.42\n",
    "\n",
    "errorMinimizer:\n",
    "  PointToPlaneErrorMinimizer:\n",
    "\n",
    "transformationCheckers:\n",
    "  - DifferentialTransformationChecker:\n",
    "      minDiffRotErr: 0.001\n",
    "      minDiffTransErr: 0.01\n",
    "      smoothLength: 2\n",
    "  - CounterTransformationChecker:\n",
    "      maxIterationCount: 100\n",
    "  - BoundTransformationChecker:\n",
    "      maxRotationNorm: 0.5\n",
    "      maxTranslationNorm: 2\n",
    "      \n",
    "inspector:\n",
    "  NullInspector\n",
    "\n",
    "logger:\n",
    "  FileLogger\n",
    "```\n",
    "At the top of the file, the kind of *matcher* to use is chosen. Matchers were introduced as association solvers in the [association lesson](../registration/2-lesson_association.ipynb) of the previous module. Here, a `KDTreeMatcher` matching with the 10 closest neighbors of each point is chosen. Then, just below, the kind of outlier filter is chosen. Outlier filters were covered in the [outliers lesson](../registration/3-lesson_outliers.ipynb) of the previous module. After that, the `PointToPlaneErrorMinimizer` is set as error minimizer. This kind of error minimizer minimizes the distance between a point in the reading cloud and the plane which is the closest in the reference cloud. Error minimizers were also covered in the previous module, in the [error minimizer lesson](../registration/4-lesson_error_minimization.ipynb). Finally, convergence conditions are stated in the `transformationCheckers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map post filters\n",
    "Here is the non-commented content of `darpa_mapPost_filters_ctu.yaml`:\n",
    "```\n",
    "- SurfaceNormalDataPointsFilter:\n",
    "    knn: 15\n",
    "\n",
    "- CutAtDescriptorThresholdDataPointsFilter:\n",
    "    descName: probabilityDynamic\n",
    "    useLargerThan: 1\n",
    "    threshold: 0.5\n",
    "```\n",
    "The first filter is used to re-compute the normals for each point of the map. Then, points which were marked as dynamic (i.e. with a probability of being dynamic higher than 50%) are removed from the map. Now that you understand how the mapper works, it is time to try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "In the terminal of your virtual machine, run the following commands:\n",
    "```\n",
    "cd ~/percep3d_data/\n",
    "wget -O bag.zip https://ulavaldti-my.sharepoint.com/:u:/g/personal/vlkub_ulaval_ca/EfejYPI79DJDiiT4rG7Mc-gBXOzOcBjJDHLm7ETSTu_Qjg?download=1\n",
    "unzip bag.zip\n",
    "rm bag.zip\n",
    "```\n",
    "If everything goes as expected, you should end up with a file named `ugv_ctu_2020-02-26-19-44-51_alpha_course_percep3d.bag` in your `~/percep3d_data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the mapping\n",
    "In the terminal of your virtual machine, start a roscore:\n",
    "```\n",
    "roscore\n",
    "```\n",
    "Then, in another tab (press `CTRL+SHIFT+T` to open a new tab), run the following commands:\n",
    "```\n",
    "rosparam set use_sim_time true\n",
    "roslaunch ethzasl_icp_mapper darpa_dynamic_mapper_ctu.launch\n",
    "```\n",
    "This will start the mapping node. However, nothing will happen because no data is published yet. In another tab, open rviz:\n",
    "```\n",
    "rviz\n",
    "```\n",
    "We will configure it to allow you to see better what is going on when the mapping starts. First, remove the grid display, we won't need it. Then click on the `add` button, choose the `By topic` tab and add the `Odometry` display under `/icp_odom`. Do the same thing again for the `PointCloud2` display under `/point_map`. Then, in the options of the `PointCloud2` display you just added, change the `Style` option to `Points`, the `Size (Pixels)` option to `2` and the `Alpha` option to `0.3`. Your rviz window should look similar to this:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/rviz.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "Finally, in a new tab of the terminal, let the magic happen:\n",
    "```\n",
    "rosbag play ~/percep3d_data/ugv_ctu_2020-02-26-19-44-51_alpha_course_percep3d.bag --clock\n",
    "```\n",
    "NB If your computer is having a hard time, you can add the `--rate=0.5` to the command above to make the bagfile play at half of its rate.\n",
    "\n",
    "It might take a few seconds before you see anything in rviz, but then you should see a map being built. The bagfile which is playing is a snippet from a DARPA competition. It lasts 50 minutes in total and contains point clouds of multiple floors that the robot explored. It is only after approximatively 2 minutes that the robot starts going down the stairs. You don't need to play the whole bagfile, just go explore the map and have some fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the map\n",
    "Once you are satisfied with the map, you can export it to a vtk file by running the following command in a new tab of the terminal:\n",
    "```\n",
    "rosservice call /dynamic_mapper/save_map \"filename:\n",
    "  data: '/home/student/Desktop/map.vtk'\"\n",
    "```\n",
    "\n",
    "Once the map is done being saved, you can open it in Paraview and play with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "You should do the following activities to enhance your understanding of the concepts viewed in this lesson:\n",
    "- modify the markdown by adding your own notes using `> my notes`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
